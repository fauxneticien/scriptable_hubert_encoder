defaults:
  - _self_
  - encoder: default.yaml

warmup_iters: 5
num_iters: 100
output_csv: ???

env:
  # Force use of single GPU if in multi-GPU environment
  CUDA_VISIBLE_DEVICES: "0"

data:
  _target_: components._helpers.MiniLibriSpeech
  train_data_dir: 'data/mini-librispeech/train-clean-5'
  sampler_kwargs:
    max_duration: 60
  dloader_kwargs:
    num_workers: 8
    pin_memory: True

masking:
  mask_prob: 0.8
  mask_length: 10
  mask_type: static
  mask_other: 0.0
  min_masks: 2
  no_overlap: False
  min_space: 1

num_pretraining_targets: 512

loss:
  _target_: components.loss.HuBERTPretrainingLoss

optim:
  max_lr: 1e-4
  max_updates: 100_000

  optimizer:
    _target_: torch.optim.AdamW
    lr: ${optim.max_lr}

  scheduler:
    _target_: torch.optim.lr_scheduler.OneCycleLR
    total_steps: ${optim.max_updates}
    max_lr: ${optim.max_lr}
    anneal_strategy: linear
